{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Raw Data From Stack Overflow\n",
    "\n",
    "Systematically extracting business intelligence from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inline, interactivate plots\n",
    "%matplotlib inline\n",
    "\n",
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is our question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially: What are the biggest problem areas for Python programmers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify the data (Stack Overflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: All the questions on Stack Overflow that have the \"Python\" tag on them.\n",
    "\n",
    "Question: Where exactly does all of this data live -- what is the URL structure we can use to \n",
    "acquire all of this user data?\n",
    "\n",
    "Task: Work the URL into a formattable string template you can feed into a scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SO_URL = \"https://stackoverflow.com/questions/tagged/python?page={0}&sort=frequent&pagesize=50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_pages_to_gather = 50\n",
    "\n",
    "# create sequence of page\n",
    "page_range = range(1, number_of_pages_to_gather + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you may have noticed when we were formatting the URL that there are actually 1000s of pages of Python questions,\n",
    "but here we're only collecting 50. This is intentional and temporary. Eventually we should collect the entire corpus of data, but right now we are trying to prototype a workflow.  So we are going to temporarily **downsample** to more rapidly prototype.\n",
    "\n",
    "As a matter of fact, 50 is pretty high.  Let's kick it down to 5 files.  That way, we are still coding with the for conditions where we need to take multiple files (as opposed to just one), but not introducing lots of computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "page_range = range(1, 6)  \n",
    "print(page_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MAKE A PAGE URLS GENERATOR (argument: array of numbers, yields URLs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "def http_get (URL):\n",
    "    response = requests.get(URL)\n",
    "    return response\n",
    "\n",
    "# \n",
    "#  Once you have the data, it can be helpful to comment the following loop out.\n",
    "# \n",
    "for i in page_range:\n",
    "    so_response = http_get(SO_URL.format(i))\n",
    "    \n",
    "    if so_response.status_code == 200:\n",
    "        html_file = open('FILENAME_00{0}.html'.format(i),'w')\n",
    "        html_file.write(so_response.text.encode('ascii', 'ignore'))\n",
    "        html_file.close()\n",
    "    else:\n",
    "        print(\"Failed at loop: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mash Until No Good! Data Munging/Wrangling/Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bad programmers worry about the code. \n",
    ">\n",
    "> Good programmers worry about data structures and their relationships.\n",
    ">\n",
    "> Linus Torvalds, creator of Linux and git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I be in the kitchen whipping\n",
    ">\n",
    "> trying to cook the sauce.\n",
    ">\n",
    ">   Yo Gotti, _The Art of the Hustle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to begin whipping this data into shape for various levels of analysis - it's hard to do \n",
    "analysis on a bunch of data locked up in an HTML structure though.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the maximum number of dimensions from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the stack overflow page and think about what our granular data points are.  For the pages\n",
    "we have decided to \n",
    "\n",
    "< INSERT PICTURE OF SO PAGE HERE >\n",
    "\n",
    "The granular logical data point is a question.  So what are the dimensions/attributes of a question object?\n",
    "- question text\n",
    "- vote score\n",
    "- views \n",
    "- details\n",
    "- author\n",
    "- question details \n",
    "\n",
    "Beautiful soup parses the HTML into a Python tree structure (DOM).  You can then use a variety of BS4 methods to extract specific HTML elements based on HTML attritbute.  Since classes and IDs are HTML attributes, you can use CSS selectors to extract information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert quick BS4 demo before doing the real code in the next block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's actually get the question text, vote score, views, etc. out of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILENAME_001.html\n",
      "FILENAME_002.html\n",
      "FILENAME_003.html\n",
      "FILENAME_004.html\n",
      "FILENAME_005.html\n"
     ]
    }
   ],
   "source": [
    "for i in page_range:\n",
    "    file_name = 'FILENAME_00{0}.html'.format(i)\n",
    "    print(file_name)\n",
    "    \n",
    "    with open(file_name,'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        \n",
    "        # for each file, get all of the question objects\n",
    "        questions = soup.find_all(\"div\", class_=\"question-summary\")\n",
    "        \n",
    "        for question in questions:\n",
    "            text = question.find('a', class_=\"question-hyperlink\").text\n",
    "            tags = [tag.text for tag in question.find_all('a', class_=\"post-tag\")]\n",
    "            views = int(question.find('div', class_=\"views\")['title'].split(\" \")[0].replace(\",\", \"\"))\n",
    "#             print(tags)\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we have figured out how to get at the data with Beautiful Soup above.  Let's wrap all of that logic into a **function** that accepts an HTML file as an argument and returns a sequence of question objects -- each object will contain all of the attributes.  Each of these will become a row in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_info_from_summary(summary_div):\n",
    "    qid = summary_div['id'].split(\"-\")[2]\n",
    "    text = summary_div.find('a', class_=\"question-hyperlink\").text\n",
    "    tags = [tag.text for tag in summary_div.find_all('a', class_=\"post-tag\")]\n",
    "    views = int(summary_div.find('div', class_=\"views\")['title'].split(\" \")[0].replace(\",\", \"\"))\n",
    "\n",
    "    # data isn't always there\n",
    "    date = summary_div.find('span', class_='relativetime')\n",
    "    date_asked = date['title'] if date else None\n",
    "    \n",
    "    return [qid, text, tags, views, date_asked]\n",
    "\n",
    "\n",
    "def extract_question_objects(relative_html_path):\n",
    "    \"\"\"\n",
    "        :relative_html_path: file to read and parse for stack overflow questions\n",
    "    \"\"\"\n",
    "    questions_objects = []\n",
    "    with open(relative_html_path, 'r') as f:\n",
    "    \n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        question_divs = soup.find_all(\"div\", class_=\"question-summary\")\n",
    "\n",
    "        for question in question_divs:\n",
    "                q_info = get_question_info_from_summary(question)\n",
    "                questions_objects.append(q_info)\n",
    "    \n",
    "    \n",
    "    return questions_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in page_range:\n",
    "    filename = \"FILENAME_00{}.html\".format(i)\n",
    "    \n",
    "    qs = extract_question_objects(filename)\n",
    "    \n",
    "    for q in qs:\n",
    "        dataset.extend(q)\n",
    "\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this raw data looks pretty clean.  It's time to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter pandas, the lingua franca of data analysis.  It works very well with tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# dataset2 = {}\n",
    "\n",
    "# # munge dict of lists into pd dataframe-acceptable format\n",
    "# for q in dataset:\n",
    "#     dataset2[]\n",
    "\n",
    "df = pd.DataFrame(columns=['views', 'text', 'tags'], )\n",
    "\n",
    "for data in dataset:\n",
    "    qid, views, text, tags = data\n",
    "    df.loc[qid] = [views, text, tuple(np.array(tags))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exploratory Data Analysis: Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing is its own special area.  One of the first things people often do is make a word cloud.  To do that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter({'k': 12, 'k': 15})\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in df.iterrows():\n",
    "    print i, text.text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: Tag Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another data frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
